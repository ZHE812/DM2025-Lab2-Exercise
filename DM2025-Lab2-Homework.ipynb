{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name:李政哲\n",
    "\n",
    "Student ID:411580398\n",
    "\n",
    "GitHub ID:ZHE812\n",
    "\n",
    "Kaggle name:FJU_411580398\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "![pic_ranking.png.png](./pics/5.png)\n",
    "![pic_ranking.png.png](./pics/6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project Report**\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "\n",
    "**In this section, I describe the advanced data preprocessing, comprehensive feature engineering, and robust model selection process that led to a public score of 0.6239.**\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "在資料前處理階段，主要的挑戰在於處理巢狀結構的 JSON 檔案以及整合多個來源的數據。\n",
    "\n",
    "* **JSON Flattening (資料攤平)：**\n",
    "  原始的 `final_posts.json` 檔案並非標準的列表格式，而是具有多層巢狀結構 (`root -> _source -> post`)。我使用了 List Comprehension 技巧，遍歷 JSON 結構並提取最深層的 `post` 字典，將其轉換為平面的 Pandas DataFrame，確保 `post_id` 與 `text` 能夠被正確讀取。\n",
    "\n",
    "* **Data Merging (資料合併)：**\n",
    "  透過 `post_id` 作為鍵值 (Key)，將文本資料 (`df_posts`)、資料分組資訊 (`df_id`) 以及訓練集的情緒標籤 (`df_emotion`) 進行合併。\n",
    "    * 與 `df_id` 進行 **inner join** 以確定每筆資料屬於訓練集或測試集。\n",
    "    * 與 `df_emotion` 進行 **left join**，保留測試集的資料（標籤為 `NaN`）。\n",
    "\n",
    "* **Missing Values Check：**\n",
    "  合併後檢查了資料完整性，確認訓練集沒有缺失值 (Missing Values)，確保模型輸入的品質。\n",
    "\n",
    "* **Text Cleaning & Hashtag Extraction:**\n",
    "  為了確保文本品質，我執行了標準的文本清理步驟。特別地，我設計了一個 `merge_hashtags` 函數，不僅將 `hashtags` 欄位轉換為字符串並附加到原始文本的末尾，同時也將其從文本中提取出來，讓模型能從這些強烈的情緒標籤中學習。此舉是為了確保 TF-IDF 在捕捉關鍵字時，能將顯性的情緒提示納入考量。\n",
    "\n",
    "![Preprocessing Flowchart](./pics/1.png)\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "為將原始文本轉換為機器學習模型可理解的數值特徵，我採取了**雙軌並進 (Dual-Path)** 的高階特徵工程策略，這是本次取得高分的關鍵之一：\n",
    "\n",
    "* **Word-level TF-IDF Vectorization (詞彙級特徵)：**\n",
    "  這是核心的語義捕捉方法。我使用 `TfidfVectorizer` 將文本轉換為稀疏矩陣。\n",
    "    * **Stopwords Removal：** 移除了英文停用詞 (`English Stop Words`) 以減少雜訊。\n",
    "    * **N-grams (1, 3)：** 為了捕捉更複雜的語義模式和詞組，我將 N-gram 範圍擴展至 **Unigrams, Bigrams, 和 Trigrams (`ngram_range=(1, 3)`)**。這有助於模型理解如 \"I am not happy\" 這類帶有否定詞或特定修飾詞的情緒表達。\n",
    "    * **Sublinear TF:** 應用 `sublinear_tf=True` 對詞頻進行對數縮放，有效平滑化高頻詞的影響，使其不會過度主導特徵空間。\n",
    "    * **Dimension Expansion：** 為容納更豐富的 N-gram 特徵，特徵維度提升至 **25,000**。\n",
    "\n",
    "* **Character-level TF-IDF Vectorization (字元級特徵)：**\n",
    "  針對社交媒體文本的特性（如拼寫錯誤、縮寫、網路流行語、重複字母等），我引入了字元級 N-grams。\n",
    "    * **Analyzer='char'：** 將 TF-IDF 的分析單位從「詞彙」切換到「字元」。\n",
    "    * **N-grams (2, 5)：** 捕捉 2 到 5 個連續字元的模式，例如 `soooo` 會被解析為 `so`, `oo`, `ooo`, `oood` 等，這對於模型識別情緒語氣、流行語的變體尤其有效。\n",
    "    * **Dimension Expansion：** 為捕捉豐富的字元模式，特徵維度設定為 **30,000**。\n",
    "\n",
    "* **Feature Union (特徵融合)：**\n",
    "  最終，我使用 `scipy.sparse.hstack` 將上述兩種 TF-IDF 特徵矩陣**水平堆疊 (Horizontally Stacked)**。這使得模型能夠同時從宏觀的詞彙語義和微觀的字元結構中學習，形成一個擁有 **高達 55,000 維度 (25000 + 30000)** 的綜合特徵空間，極大地增強了模型的表達能力和魯棒性。\n",
    "\n",
    "![Feature Engineering Diagram](./pics/2.png)\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "在模型選擇與優化過程中，我採取了**集成學習 (Ensemble Learning)** 的策略，並進一步升級為**軟投票 (Soft Voting)**，結合了四種不同特性與優勢的基學習器，以達到最佳預測效果：\n",
    "\n",
    "* **Baseline Model (Logistic Regression)：**\n",
    "  最初使用標準的 Logistic Regression 進行訓練。雖然運算速度快，但由於資料類別分佈極不平均（**Joy** 類別佔絕大多數），模型傾向於預測多數類別，導致 Mean F1 Score 約落在 0.55 左右。此模型在後續的集成中仍因其穩健性和機率輸出能力而保留。\n",
    "\n",
    "* **Handling Class Imbalance：**\n",
    "  為改善對少數類別（如 **Disgust**, **Fear**）的預測，我對所有基學習器加入了 `class_weight='balanced'` 參數，讓模型在訓練時能更關注少數樣本。\n",
    "\n",
    "* **Advanced Models for Ensemble：**\n",
    "  為了構建強大的集成，我嚴選了四種性能卓越且互補的分類器：\n",
    "\n",
    "    1. **Calibrated LinearSVC (強大的 SVM + 機率校準)**：\n",
    "       `LinearSVC` 在處理高維度稀疏特徵時表現優異。為使其能參與軟投票 (需要機率輸出)，我將其用 `CalibratedClassifierCV(method='sigmoid', cv=5)` 進行包裝和機率校準。這使得 `LinearSVC` 不僅能找到最佳分類超平面，還能輸出可靠的預測機率。\n",
    "\n",
    "    2. **Logistic Regression (穩健的機率模型)**：\n",
    "       作為線性模型，其解釋性強且效能穩健。我將 `C` 值調整為 0.9，並使用 `solver='saga'` 以更好地處理高維特徵和 L1/L2 正則化。\n",
    "\n",
    "    3. **Multinomial Naive Bayes (文本計數專家)**：\n",
    "       基於貝氏定理，此模型對文本計數特徵有天然優勢。我將平滑參數 `alpha` 調低至 `0.05`，使其對稀疏且高維的特徵更為敏感。\n",
    "\n",
    "    4. **SGDClassifier (Modified Huber Loss) (高效的線性分類器)**：\n",
    "       引入 `SGDClassifier`，它使用隨機梯度下降優化，並採用 `loss='modified_huber'`。這種損失函數結合了hinge loss (SVM) 和 log loss (LR) 的優點，能原生支持機率輸出，並在處理大規模、高維度數據時提供互補的性能和角度。\n",
    "\n",
    "* **Final Ensemble (Soft Voting Classifier)：**\n",
    "  這是最終的決策層，將四個基學習器的預測結果進行集成。\n",
    "    * **`voting='soft'` (軟投票)**：採用「機率加權平均」策略。每個基學習器會輸出對各類別的信心分數（機率），`VotingClassifier` 則會根據這些機率和預設權重進行加權平均，選擇機率最高的類別。這比傳統的多數決 `hard voting` 更能捕捉模型的信心程度。\n",
    "    * **`weights=[2, 2, 1, 1.5]` (權重分配)**：根據經驗和實驗結果，我為 `Calibrated LinearSVC` 和 `Logistic Regression` 分配了較高的權重 (2)，因為它們通常表現最為穩定和準確。`SGDClassifier` 獲得中等權重 (1.5)，而 `MultinomialNB` 則獲得較低權重 (1)，作為補充性的視角。\n",
    "    * **Cross-Validation (交叉驗證)**：在集成模型訓練前，我使用 `StratifiedKFold` 進行 5-Fold 交叉驗證，以確保模型性能的穩定性和泛化能力，避免過度擬合特定數據切分。\n",
    "\n",
    "這種結合了**雙重特徵流**與**多樣化且精準調校的軟投票集成模型**，最終取得了 **0.6239** 的 Public Score，大幅超越了先前的所有嘗試。\n",
    "\n",
    "![Model Architecture](./pics/3.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonus Section\n",
    "\n",
    "**Add more detail in previous sections**\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "\n",
    "在模型開發過程中，我進行了大量的實驗和探索，以下是幾個關鍵的嘗試及其發現：\n",
    "\n",
    "* **Class Weight Experiment：** 嘗試調整 Logistic Regression 的 `C` 值與 `class_weight`，發現對於極度不平衡的資料集，單純加重權重有時會造成 False Positives 暴增。最終確定在所有基模型中啟用 `class_weight='balanced'` 是最穩定的做法。\n",
    "\n",
    "* **Feature Dimension & N-grams：**\n",
    "    * 比較了 5000 維、10000 維、15000 維以及最終 25000 維的詞彙特徵空間，發現隨著特徵維度的增加（尤其是在引入 N-grams 後），模型性能有顯著提升，但超過一定數量後效果會出現邊際遞減。\n",
    "    * 從 Unigrams 逐漸擴展到 Bigrams (`ngram_range=(1,2)`)，再到最終的 Trigrams (`ngram_range=(1,3)`)，每次擴展都帶來了 F1 Score 的提升，尤其 Trigrams 對於捕捉情緒的完整語義非常關鍵。\n",
    "\n",
    "* **Model Comparison (Individual)：**\n",
    "    * 實測比較了 Naive Bayes、Logistic Regression 與 LinearSVC，確認 LinearSVC 在單一模型中表現最佳。\n",
    "    * 後期引入的 `SGDClassifier` 在交叉驗證中也展現出良好的性能，證明了其作為集成基學習器的潛力。\n",
    "\n",
    "* **Voting Strategy Exploration：**\n",
    "    * 早期嘗試了 `hard voting`，雖然比單一模型有所提升，但效果有限。\n",
    "    * 轉向 `soft voting` 後，尤其是在 `CalibratedClassifierCV` 包裝 `LinearSVC` 並確保所有模型都能輸出可靠機率後，模型性能有了質的飛躍，證明了考慮模型「信心程度」的重要性。\n",
    "\n",
    "* **Feature Union 嘗試：**\n",
    "    * 最初只使用 Word-level TF-IDF，後續嘗試將 Character-level TF-IDF 與 Word-level 特徵融合，這是最終突破 0.60 分數線的關鍵一步。字符級特徵有效處理了社交媒體文本的特殊性。\n",
    "\n",
    "![Comparison Chart](./pics/4.png)\n",
    "\n",
    "### 2.2 Mention Insights You Gained\n",
    "\n",
    "這場競賽讓我對文本分類和集成學習有了更深刻的理解：\n",
    "\n",
    "* **數據不平衡的挑戰與解決方案：**\n",
    "  儘管 `class_weight='balanced'` 有幫助，但單靠它不足以解決所有問題。真正有效的解決方案是結合魯棒的特徵（如 Hashtags、N-grams）和性能強大的模型，讓模型能夠從多個角度識別少數類別的模式。觀察 Classification Report 中 \"Joy\" (喜悅) 的 F1-score 雖然高達 0.79，但 \"Disgust\" (厭惡) 僅為 0.15 的現象，正是這種不平衡導致的。\n",
    "\n",
    "* **特徵工程是決定分數上限的關鍵：**\n",
    "  從僅僅添加 Hashtags 到擴展 N-grams，再到最終結合 Word-level 和 Character-level 特徵，每一步特徵工程的精進都帶來了實質性的性能提升。這遠比單純調整模型參數或更換模型類型來得有效。特別是 Character-level N-grams，它有效地解決了社交媒體文本中不規範用語的問題，證明了對數據特性的深入理解和特徵構造的重要性。\n",
    "\n",
    "* **集成學習的力量與多樣性：**\n",
    "  集成學習的優勢不在於單一模型的強大，而在於「多樣性」。結合不同原理的模型（如 SVM 的最大間隔、LR 的機率、NB 的貝氏、SGD 的梯度下降），並通過軟投票策略，能夠有效降低單一模型的偏見和方差。特別是通過 `CalibratedClassifierCV` 讓 `LinearSVC` 能夠輸出機率，是實現高效軟投票的關鍵，讓集成模型能夠做出更精準、更有信心的決策。這次從 0.5852 提升到 0.6239 的飛躍，正是由這兩點（特徵融合與更強的軟投票集成）所共同驅動的。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在讀取 final_posts.json ...\n",
      "JSON 讀取完成，共 64171 筆。\n",
      "欄位範例: ['post_id', 'text', 'hashtags']\n",
      "\n",
      "正在讀取 CSV 檔案 ...\n",
      "\n",
      "正在合併資料 ...\n",
      "合併完成！\n",
      "    post_id                                               text hashtags  \\\n",
      "0  0x61fc95  We got the ranch, loaded our guns and sat up t...       []   \n",
      "1  0x35663e  I bet there is an army of married couples who ...       []   \n",
      "2  0xc78afe                         This could only end badly.       []   \n",
      "\n",
      "   split emotion  \n",
      "0   test     NaN  \n",
      "1  train     joy  \n",
      "2  train    fear  \n",
      "\n",
      "最後資料集檢查:\n",
      "Training set 數量: 47890 (應有 emotion 標籤)\n",
      "Testing set 數量 : 16281 (emotion 應為 NaN)\n",
      "\n",
      "訓練集缺失值檢查:\n",
      "text       0\n",
      "emotion    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 1. 讀取並處理 JSON 文本資料\n",
    "print(\"正在讀取 final_posts.json ...\")\n",
    "with open('final_posts.json', 'r', encoding='utf-8') as f:\n",
    "    posts_data = json.load(f)\n",
    "\n",
    "# 解析巢狀結構：從 root -> _source -> post 中提取資料\n",
    "# 使用 List Comprehension 快速提取\n",
    "processed_data = [item['root']['_source']['post'] for item in posts_data]\n",
    "\n",
    "# 轉成 DataFrame\n",
    "df_posts = pd.DataFrame(processed_data)\n",
    "print(f\"JSON 讀取完成，共 {len(df_posts)} 筆。\")\n",
    "print(\"欄位範例:\", df_posts.columns.tolist())\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 2. 讀取 CSV 標籤與分組資料\n",
    "# ---------------------------\n",
    "print(\"\\n正在讀取 CSV 檔案 ...\")\n",
    "df_emotion = pd.read_csv('emotion.csv')\n",
    "df_id = pd.read_csv('data_identification.csv')\n",
    "\n",
    "# 重新命名欄位以利合併 (統一改為 'post_id')\n",
    "df_emotion = df_emotion.rename(columns={'id': 'post_id'})\n",
    "df_id = df_id.rename(columns={'id': 'post_id'})\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 3. 資料合併 (Merge)\n",
    "# ---------------------------\n",
    "print(\"\\n正在合併資料 ...\")\n",
    "# 步驟 A: 將文本 (df_posts) 與 分組資訊 (df_id) 合併\n",
    "df_full = pd.merge(df_posts, df_id, on='post_id', how='inner')\n",
    "\n",
    "# 步驟 B: 將結果與 情緒標籤 (df_emotion) 合併\n",
    "# 測試集的資料在 df_emotion 裡沒有對應，所以用 how='left' 保留所有資料\n",
    "df_full = pd.merge(df_full, df_emotion, on='post_id', how='left')\n",
    "\n",
    "print(\"合併完成！\")\n",
    "print(df_full.head(3))\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 4. 分割訓練集與測試集\n",
    "# ---------------------------\n",
    "# 根據 'split' 欄位進行分割\n",
    "train_df = df_full[df_full['split'] == 'train'].copy()\n",
    "test_df = df_full[df_full['split'] == 'test'].copy()\n",
    "\n",
    "print(f\"\\n最後資料集檢查:\")\n",
    "print(f\"Training set 數量: {len(train_df)} (應有 emotion 標籤)\")\n",
    "print(f\"Testing set 數量 : {len(test_df)} (emotion 應為 NaN)\")\n",
    "\n",
    "# 檢查訓練集是否有缺失值\n",
    "print(\"\\n訓練集缺失值檢查:\")\n",
    "print(train_df[['text', 'emotion']].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特徵矩陣形狀: (47890, 5000)\n"
     ]
    }
   ],
   "source": [
    "### Add the code related to the preprocessing steps in cells inside this section\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 初始化 TF-IDF\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "# 擬合並轉換訓練集，只轉換測試集\n",
    "X_train = tfidf.fit_transform(train_df['text'])\n",
    "X_test = tfidf.transform(test_df['text'])\n",
    "y_train = train_df['emotion']\n",
    "\n",
    "print(\"特徵矩陣形狀:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在將 Hashtags 合併至文字特徵中...\n",
      "合併完成！\n",
      "範例 (原本): It’s the NRA, I don’t think it should shock anyone regardless.\n",
      "範例 (新版): It’s the NRA, I don’t think it should shock anyone regardless.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 特徵工程策略：合併 Hashtags\n",
    "# ---------------------------\n",
    "# 原始資料中有 hashtags 欄位，裡面可能包含情緒關鍵字\n",
    "# 合併到 text 欄位中，讓 TF-IDF 也能捕捉到這些特徵\n",
    "\n",
    "def merge_hashtags(row):\n",
    "    # 檢查 hashtags 是否為列表且不為空\n",
    "    if isinstance(row['hashtags'], list) and len(row['hashtags']) > 0:\n",
    "        # 將 hashtags 串接在原始文字後面\n",
    "        return row['text'] + \" \" + \" \".join(row['hashtags'])\n",
    "    else:\n",
    "        return row['text']\n",
    "\n",
    "print(\"正在將 Hashtags 合併至文字特徵中...\")\n",
    "\n",
    "# 應用到 train_df 和 test_df\n",
    "# 創建一個新欄位 text_full 來存放合併後的結果\n",
    "train_df['text_full'] = train_df.apply(merge_hashtags, axis=1)\n",
    "test_df['text_full'] = test_df.apply(merge_hashtags, axis=1)\n",
    "\n",
    "print(\"合併完成！\")\n",
    "print(\"範例 (原本):\", train_df['text'].iloc[100])\n",
    "print(\"範例 (新版):\", train_df['text_full'].iloc[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在進行 TF-IDF 轉換 ...\n",
      "特徵工程完成！\n",
      "訓練集矩陣形狀 (X_train): (47890, 5000)\n",
      "測試集矩陣形狀 (X_test): (16281, 5000)\n"
     ]
    }
   ],
   "source": [
    "### Add the code related to the feature engineering steps in cells inside this section\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 初始化 TF-IDF 向量化器\n",
    "# max_features=5000 表示只取出現頻率最高的前 5000 個字，避免特徵過多\n",
    "# stop_words='english' 會移除像 is, the, at 這種無意義的英文停用詞\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "print(\"正在進行 TF-IDF 轉換 ...\")\n",
    "\n",
    "# 1. 擬合 (Fit) 並轉換 (Transform) 訓練集文字\n",
    "X_train = tfidf.fit_transform(train_df['text'])\n",
    "\n",
    "# 2. 只轉換 (Transform) 測試集文字 (注意：測試集不能參與 Fit，這叫 Data Leakage)\n",
    "X_test = tfidf.transform(test_df['text'])\n",
    "\n",
    "# 準備標籤 (y)\n",
    "y_train = train_df['emotion']\n",
    "\n",
    "print(\"特徵工程完成！\")\n",
    "print(f\"訓練集矩陣形狀 (X_train): {X_train.shape}\")\n",
    "print(f\"測試集矩陣形狀 (X_test): {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在初始化模型...\n",
      "正在訓練模型 (Training)... 這可能需要幾秒鐘\n",
      "模型訓練完成！\n",
      "\n",
      "訓練集分類報告 (Classification Report):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.63      0.62      0.62     10694\n",
      "     disgust       0.62      0.09      0.15      1183\n",
      "        fear       0.71      0.36      0.48      2009\n",
      "         joy       0.69      0.91      0.79     23797\n",
      "     sadness       0.65      0.32      0.43      3926\n",
      "    surprise       0.62      0.31      0.42      6281\n",
      "\n",
      "    accuracy                           0.67     47890\n",
      "   macro avg       0.65      0.43      0.48     47890\n",
      "weighted avg       0.66      0.67      0.64     47890\n",
      "\n",
      "正在對測試集進行預測 (Predicting)...\n",
      "\n",
      "預測檔案已建立：submission.csv\n",
      "前 5 筆預測結果:\n",
      "         id emotion\n",
      "0  0x61fc95   anger\n",
      "4  0xaba820     joy\n",
      "5  0x66e44d     joy\n",
      "6  0xc03cf5     joy\n",
      "8  0x02f65a     joy\n"
     ]
    }
   ],
   "source": [
    "### Add the code related to the model implementation steps in cells inside this section\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------\n",
    "# 1. 建立與訓練模型\n",
    "# ---------------------------\n",
    "# 使用 Logistic Regression 作為 Baseline 模型\n",
    "# max_iter=1000 確保模型有足夠時間收斂\n",
    "print(\"正在初始化模型...\")\n",
    "model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "\n",
    "print(\"正在訓練模型 (Training)... 這可能需要幾秒鐘\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"模型訓練完成！\")\n",
    "\n",
    "# ---------------------------\n",
    "# 2. 自我評估 (Optional)\n",
    "# ---------------------------\n",
    "# 看模型在訓練集上的表現 (供參)\n",
    "# 如分數低，代表模型 Underfitting\n",
    "train_pred = model.predict(X_train)\n",
    "print(\"\\n訓練集分類報告 (Classification Report):\")\n",
    "print(classification_report(y_train, train_pred))\n",
    "\n",
    "# ---------------------------\n",
    "# 3. 預測測試集 (Testing)\n",
    "# ---------------------------\n",
    "print(\"正在對測試集進行預測 (Predicting)...\")\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. 產生提交Submission\n",
    "# ---------------------------\n",
    "# 根據要求的格式：id, emotion\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['post_id'],\n",
    "    'emotion': test_predictions\n",
    "})\n",
    "\n",
    "submission_filename = 'submission.csv'\n",
    "submission.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"\\n預測檔案已建立：{submission_filename}\")\n",
    "print(\"前 5 筆預測結果:\")\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在訓練優化版模型 (Balanced Mode)...\n",
      "優化版模型訓練完成！\n",
      "正在預測...\n",
      "\n",
      "新的預測檔案已建立：submission_balanced.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "# 策略：使用「類別平衡 (Balanced)」模式\n",
    "\n",
    "print(\"正在訓練優化版模型 (Balanced Mode)...\")\n",
    "\n",
    "model_balanced = LogisticRegression(\n",
    "    max_iter=1000, \n",
    "    class_weight='balanced',  # <--- 這是關鍵！\n",
    "    random_state=42, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model_balanced.fit(X_train, y_train)\n",
    "print(\"優化版模型訓練完成！\")\n",
    "\n",
    "# ---------------------------\n",
    "# 預測與產生新檔案\n",
    "# ---------------------------\n",
    "print(\"正在預測...\")\n",
    "test_predictions = model_balanced.predict(X_test)\n",
    "\n",
    "submission_filename = 'submission_balanced.csv'\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['post_id'],\n",
    "    'emotion': test_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"\\n新的預測檔案已建立：{submission_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在對 'text_full' 進行 TF-IDF 轉換...\n",
      "新特徵矩陣形狀: (47890, 10000)\n",
      "正在訓練 LinearSVC 模型 (含 Hashtags 特徵)...\n",
      "模型訓練完成！\n",
      "正在預測...\n",
      "\n",
      "新的預測檔案已建立：submission_svc_hashtags.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------\n",
    "# 1. 重新執行 TF-IDF (使用 text_full)\n",
    "# ---------------------------\n",
    "print(\"正在對 'text_full' 進行 TF-IDF 轉換...\")\n",
    "tfidf = TfidfVectorizer(max_features=10000, stop_words='english', ngram_range=(1, 2))\n",
    "\n",
    "# 改成使用 'text_full'\n",
    "X_train_new = tfidf.fit_transform(train_df['text_full'])\n",
    "X_test_new = tfidf.transform(test_df['text_full'])\n",
    "\n",
    "print(f\"新特徵矩陣形狀: {X_train_new.shape}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 2. 訓練 LinearSVC 模型\n",
    "# ---------------------------\n",
    "print(\"正在訓練 LinearSVC 模型 (含 Hashtags 特徵)...\")\n",
    "# 保持之前的設定 C=0.5\n",
    "model_svc = LinearSVC(C=0.5, random_state=42, max_iter=3000)\n",
    "model_svc.fit(X_train_new, y_train)\n",
    "print(\"模型訓練完成！\")\n",
    "\n",
    "# ---------------------------\n",
    "# 3. 預測與存檔\n",
    "# ---------------------------\n",
    "print(\"正在預測...\")\n",
    "test_predictions = model_svc.predict(X_test_new)\n",
    "\n",
    "submission_filename = 'submission_svc_hashtags.csv'\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['post_id'],\n",
    "    'emotion': test_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"\\n新的預測檔案已建立：{submission_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在建立 Voting Classifier (集成 LinearSVC, LR, MNB)...\n",
      "正在訓練集成模型...\n",
      "集成模型訓練完成！\n",
      "正在預測...\n",
      "\n",
      "最終預測檔案已建立：submission_voting.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "import pandas as pd\n",
    "\n",
    "# Voting Classifier (集成學習)\n",
    "# 結合 LinearSVC, Logistic Regression, MultinomialNB\n",
    "\n",
    "print(\"正在建立 Voting Classifier (集成 LinearSVC, LR, MNB)...\")\n",
    "\n",
    "# 1. 定義基模型 (Base Estimators)\n",
    "# LinearSVC: 幾何間隔最大化，對稀疏矩陣效果好\n",
    "clf1 = LinearSVC(C=0.5, random_state=42, max_iter=3000, dual='auto')\n",
    "\n",
    "# Logistic Regression: 機率模型，穩健的 Baseline\n",
    "clf2 = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "\n",
    "# MultinomialNB: 貝氏定理，適合文字計數特徵\n",
    "clf3 = MultinomialNB()\n",
    "\n",
    "# 2. 建立 Voting Classifier\n",
    "# voting='hard' 表示使用「多數決」策略\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('svc', clf1),\n",
    "        ('lr', clf2),\n",
    "        ('mnb', clf3)\n",
    "    ],\n",
    "    voting='hard',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 3. 訓練模型\n",
    "# 使用上一格生成的 X_train_new (包含 Hashtags 資訊)\n",
    "print(\"正在訓練集成模型...\")\n",
    "voting_clf.fit(X_train_new, y_train)\n",
    "print(\"集成模型訓練完成！\")\n",
    "\n",
    "# 4. 預測\n",
    "print(\"正在預測...\")\n",
    "test_predictions = voting_clf.predict(X_test_new)\n",
    "\n",
    "# 5. 存檔\n",
    "submission_filename = 'submission_voting.csv'\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['post_id'],\n",
    "    'emotion': test_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv(submission_filename, index=False)\n",
    "print(f\"\\n最終預測檔案已建立：{submission_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在啟動終極模型配置 (Soft Voting Mode)...\n",
      "1. 正在生成 TF-IDF 特徵 (1-3 grams, Sublinear TF)...\n",
      "   特徵矩陣形狀: (47890, 20000)\n",
      "2. 初始化並校準基模型...\n",
      "3. 正在進行 5-Fold 交叉驗證 (這需要一點時間)...\n",
      "   交叉驗證 F1 Score 平均值: 0.5605 (std: 0.0049)\n",
      "4. 正在訓練最終 Soft Voting 集成模型...\n",
      "模型訓練完成！\n",
      "5. 正在預測測試集...\n",
      "\n",
      "預測檔案已建立：submission_voting_soft_trigram.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Ultimate Model: Soft Voting + Calibration + Cross-Validation\n",
    "\n",
    "print(\"正在啟動終極模型配置 (Soft Voting Mode)...\")\n",
    "\n",
    "# 1. 特徵工程：開啟 Trigrams 與 Sublinear TF\n",
    "print(\"1. 正在生成 TF-IDF 特徵 (1-3 grams, Sublinear TF)...\")\n",
    "# sublinear_tf=True 對頻率進行對數縮放，對長文本或情緒分析通常有效\n",
    "tfidf_ultimate = TfidfVectorizer(\n",
    "    max_features=20000,       # 再次提升特徵維度\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 3),       # <--- 抓到 \"I am not happy\" 這種三個字的組合\n",
    "    sublinear_tf=True         # <--- 平滑化高頻詞的影響\n",
    ")\n",
    "\n",
    "# 使用包含 Hashtags 的完整文本\n",
    "X_train_ult = tfidf_ultimate.fit_transform(train_df['text_full'])\n",
    "X_test_ult = tfidf_ultimate.transform(test_df['text_full'])\n",
    "print(f\"   特徵矩陣形狀: {X_train_ult.shape}\")\n",
    "\n",
    "\n",
    "# 2. 定義更強的基模型 (加入機率校準)\n",
    "print(\"2. 初始化並校準基模型...\")\n",
    "\n",
    "# --- Model A: Calibrated LinearSVC ---\n",
    "# LinearSVC 本身沒有 predict_proba，必須用 CalibratedClassifierCV 包起來才能進行 Soft Voting\n",
    "svc_base = LinearSVC(C=0.2, random_state=42, max_iter=3000, dual='auto', class_weight='balanced')\n",
    "clf_svc_prob = CalibratedClassifierCV(svc_base, method='sigmoid', cv=5) \n",
    "\n",
    "# --- Model B: Logistic Regression (調參版) ---\n",
    "# C 值調小一點 (0.8) 增加正則化，避免過擬合\n",
    "clf_lr = LogisticRegression(C=0.8, max_iter=2000, random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "\n",
    "# --- Model C: MultinomialNB (高靈敏度版) ---\n",
    "# alpha 調得非常小，讓它對特徵更敏感\n",
    "clf_mnb = MultinomialNB(alpha=0.01)\n",
    "\n",
    "\n",
    "# 3. 交叉比對驗證 (Cross-Validation)\n",
    "# 這就是你要的「交叉比對」，我們把資料切成 5 份輪流驗證，看看模型穩不穩\n",
    "print(\"3. 正在進行 5-Fold 交叉驗證 (這需要一點時間)...\")\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 這裡我們簡單驗證 LR 的效果當作指標\n",
    "scores = cross_val_score(clf_lr, X_train_ult, y_train, cv=cv, scoring='f1_weighted', n_jobs=-1)\n",
    "print(f\"   交叉驗證 F1 Score 平均值: {np.mean(scores):.4f} (std: {np.std(scores):.4f})\")\n",
    "\n",
    "\n",
    "# 4. 建立 Soft Voting Classifier\n",
    "print(\"4. 正在訓練最終 Soft Voting 集成模型...\")\n",
    "# weights 可以調整，這裡我們給 SVM 和 LR 較高的權重，因為它們通常比較準\n",
    "voting_soft = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('svc_cal', clf_svc_prob),\n",
    "        ('lr', clf_lr),\n",
    "        ('mnb', clf_mnb)\n",
    "    ],\n",
    "    voting='soft',  # <--- 關鍵：使用機率加權平均\n",
    "    weights=[2, 2, 1], # 讓 SVM 和 LR 說話大聲一點\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "voting_soft.fit(X_train_ult, y_train)\n",
    "print(\"模型訓練完成！\")\n",
    "\n",
    "\n",
    "# 5. 預測與存檔\n",
    "print(\"5. 正在預測測試集...\")\n",
    "test_predictions = voting_soft.predict(X_test_ult)\n",
    "\n",
    "submission_filename = 'submission_voting_soft_trigram.csv'\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['post_id'],\n",
    "    'emotion': test_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv(submission_filename, index=False)\n",
    "print(f\"\\n預測檔案已建立：{submission_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - 正在擬合 Word Vectorizer...\n",
      "   - 正在擬合 Char Vectorizer (會比較慢)...\n",
      "   - 正在合併特徵矩陣...\n",
      "最終特徵矩陣形狀: (47890, 55000)\n",
      "2. 初始化並校準模型群...\n",
      "3. 正在進行 5-Fold 交叉驗證 (檢查 SGD 模型表現)...\n",
      "   SGD Model CV F1 Score: 0.6151 (std: 0.0049)\n",
      "4. 正在訓練最終集成模型 (4 Models)...\n",
      "模型訓練完成！\n",
      "5. 正在預測測試集...\n",
      "\n",
      "預測檔案已建立：submission_ultra_ensemble.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. 特徵工程：雙重特徵流 (Feature Union)\n",
    "\n",
    "# A. Word-level TF-IDF (捕捉語意)\n",
    "# 範圍擴大到 1-3 grams (Trigrams)\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    max_features=25000,       \n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 3),       \n",
    "    sublinear_tf=True,\n",
    "    analyzer='word'\n",
    ")\n",
    "print(\"   - 正在擬合 Word Vectorizer...\")\n",
    "train_word_features = word_vectorizer.fit_transform(train_df['text_full'])\n",
    "test_word_features = word_vectorizer.transform(test_df['text_full'])\n",
    "\n",
    "# B. Character-level TF-IDF (捕捉拼寫、語氣、表情符號結構)\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    max_features=30000,       \n",
    "    ngram_range=(2, 5),       \n",
    "    sublinear_tf=True,\n",
    "    analyzer='char'           \n",
    ")\n",
    "print(\"   - 正在擬合 Char Vectorizer (會比較慢)...\")\n",
    "train_char_features = char_vectorizer.fit_transform(train_df['text_full'])\n",
    "test_char_features = char_vectorizer.transform(test_df['text_full'])\n",
    "\n",
    "# C. 特徵融合 (Stacking Features)\n",
    "print(\"   - 正在合併特徵矩陣...\")\n",
    "X_train_combined = hstack([train_word_features, train_char_features])\n",
    "X_test_combined = hstack([test_word_features, test_char_features])\n",
    "print(f\"最終特徵矩陣形狀: {X_train_combined.shape}\")\n",
    "\n",
    "\n",
    "# 2. 定義四大模型 (Ensemble of 4)\n",
    "print(\"2. 初始化並校準模型群...\")\n",
    "\n",
    "# Model 1: Calibrated LinearSVC (幾何間隔最強)\n",
    "svc_base = LinearSVC(C=0.2, random_state=42, max_iter=3000, dual='auto', class_weight='balanced')\n",
    "clf_svc = CalibratedClassifierCV(svc_base, method='sigmoid', cv=5) \n",
    "\n",
    "# Model 2: Logistic Regression (穩健機率)\n",
    "clf_lr = LogisticRegression(C=0.9, max_iter=2000, random_state=42, n_jobs=-1, class_weight='balanced', solver='saga')\n",
    "\n",
    "# Model 3: MultinomialNB (計數特徵專家)\n",
    "# 針對超高維度，alpha 調小一點點\n",
    "clf_mnb = MultinomialNB(alpha=0.05)\n",
    "\n",
    "# Model 4: SGD Classifier (Modified Huber) \n",
    "# 它是線性模型的一種，但使用不同的 Loss function，能提供不一樣的觀點\n",
    "# loss='modified_huber' 讓它原生支援機率輸出 (predict_proba)\n",
    "clf_sgd = SGDClassifier(loss='modified_huber', penalty='l2', alpha=1e-4, random_state=42, max_iter=2000, class_weight='balanced', n_jobs=-1)\n",
    "\n",
    "\n",
    "# 3. 交叉比對驗證 (Cross-Validation)\n",
    "print(\"3. 正在進行 5-Fold 交叉驗證 (檢查 SGD 模型表現)...\")\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(clf_sgd, X_train_combined, y_train, cv=cv, scoring='f1_weighted', n_jobs=-1)\n",
    "print(f\"   SGD Model CV F1 Score: {np.mean(scores):.4f} (std: {np.std(scores):.4f})\")\n",
    "\n",
    "\n",
    "# 4. 建立 Soft Voting Classifier\n",
    "print(\"4. 正在訓練最終集成模型 (4 Models)...\")\n",
    "# 權重分配：SVM 和 LR 通常最準，給高權重；MNB 和 SGD 輔助\n",
    "voting_ultra = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('svc', clf_svc),\n",
    "        ('lr', clf_lr),\n",
    "        ('mnb', clf_mnb),\n",
    "        ('sgd', clf_sgd)\n",
    "    ],\n",
    "    voting='soft',\n",
    "    weights=[2, 2, 1, 1.5], \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "voting_ultra.fit(X_train_combined, y_train)\n",
    "print(\"模型訓練完成！\")\n",
    "\n",
    "\n",
    "# 5. 預測與存檔\n",
    "print(\"5. 正在預測測試集...\")\n",
    "test_predictions = voting_ultra.predict(X_test_combined)\n",
    "\n",
    "submission_filename = 'submission_ultra_ensemble.csv'\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['post_id'],\n",
    "    'emotion': test_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv(submission_filename, index=False)\n",
    "print(f\"\\n預測檔案已建立：{submission_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
